\chapter{Paralelismo en la biblioteca}\label{chap:par}

\begin{flushright}
  \begin{minipage}[t]{13cm}
    \begin{flushright}
      \begin{quote}
        \emph{
          So these processor manufacturers all have these nice new multi-core CPUs
           but...what's going on to get all the applications to start exploiting this? The
           magic parallelization fairy?
        }
        \begin{flushright}
          \textbf{\textemdash Grupo de noticias \texttt{comp.arch}, Nov. 2005}
        \end{flushright}
      \end{quote}
    \end{flushright}
  \end{minipage}
\end{flushright}

\bigskip


%\begin{flushright}
%  \begin{minipage}[t]{13cm}
%    \begin{flushright}
%      \begin{quote}
%        \emph{
%          Right now I'm having amnesia and déjà vu at the same time.
%        }
%        \begin{flushright}
%          \textbf{\textemdash Stephen Wright (en paralelo).}
%        \end{flushright}
%      \end{quote}
%    \end{flushright}
%  \end{minipage}
%\end{flushright}


\begin{center}{\line(1,0){325}}\end{center}

%--------------------------------------------------------%

\section{Introducción}
  Por desgracia, la respuesta a la cita que encabeza este capítulo es «no». No 
  hay ningún hada del paralelismo que vaya a hacer que los --aún hoy mayoría-- 
  programas secuenciales mágicamente aprovechen las múltiples unidades de ejecución 
  de los ubicuos procesadores multinúcleo. Por contra, recae en la comunidad, tanto
  académica como no, la revisión y en muchos casos reescritura de todos aquellos 
  algoritmos susceptibles de ser paralelizados, con el fin de efectivamente 
  conseguir aprovechar las prestaciones que ofrecen esta nueva tendencia en
  el mundo de la computación. Esto es tanto más así en los algoritmos para
  el trabajo en Matemáticas

  \bigskip

  Se ha hecho énfasis en la paralelización de los métodos básicos sobre los 
  que la mayoría de las rutinas de más alto nivel se apoyan. Por varios motivos:
  \begin{description}
    \item[Por su propia naturaleza.] Como métodos \emph{básicos}, son utilizados con 
    mucha mayor frecuencia, y por un abanico mayor de clientes. Cualquier optimización
    --en este caso la paralelización-- tendrá un efecto beneficioso directamente proporcional 
    dichos factores.
    \item[Facilidad.] La complejidad de los métodos suele ser mayor en métodos de más alto nivel.
    Por ejemplo, compárese la complejidad de los métodos de factorización de enteros con la implementación
    básica del producto de enteros. Es incluso probable que la adaptación paralela de los actuales 
    métodos secuenciales no sea posible sin una reescritura completa del método. Nótese sin embargo como,
    en base al punto anterior, incluso los complejos métodos no paralelizables sí se benefician de la paralelización
    de los básicos: siguiendo con el ejemplo de la factorización y el producto de enteros, el primer método sí se valdrá
    del segundo y será \emph{parcialmente} paralelo de forma indirecta.
    \item[Modularidad.] Un método básico es susceptible de ser reutilizado en algún otro lugar, haciendo gala de su
    modularidad. Consigo arrastrará su condición de paralelo, haciendo extensiva esta característica incluso allí 
    donde no se ha hecho un esfuerzo explícito en este sentido. 
  \end{description}




  \subsubsection{Especificación del paralelismo}
    Desgraciadamente, no existe una sintaxis universalmente aceptada para la especificación
    de algoritmos paralelos. No será en esta memoria donde se contribuya a empeorar aún más esta
    situación, sino que nos ceñiremos a la de la tecnología utilizada para la obtención
    de paralelismo en nuestros métodos, OpenMP. Esto conlleva que el lector habrá de estar al menos
    parcialmente familiarizado con la sintaxis de este interfaz. Afortunadamente, se trata de una
    familia de métodos bastante reducida y de fácil interpretación. Para una introducción de apenas
    veinte páginas --pero ya suficiente para la comprensión del código aquí incluido--, véase 
    el apéndice A de \cite{parpatterns}. En cualquier caso, en la opinión del autor, la mayoría de 
    los ejemplos de código son autoexplicativos aun utilizando las directivas de OpenMP.




\section{Producto de enteros}\label{par:karatsubaZ}
  La paralelización del método de Karatsuba para la multiplicación
  de enteros resulta sencilla: se muestra a 
 continuación la formulación de este método.
  
  \begin{equation}\label{eq:multiplicacionKaratsuba}
    X \times Y = x_1y_1(B^2 + B) + (x_1-x_0)(y_0-y_1)B + x_0y_0(B+1)
  \end{equation}
   resultando $x_0$ y $x_1$ de la división en partes iguales --baja y alta resp.-- de $x$ (razonamiento
   análogo para $y$). 

   Los sumandos de la expresión \eqref{eq:multiplicacionKaratsuba} anterior conforman tres tareas
   totalmente independientes entre sí, siendo por tanto \emph{embarazosamente paralelizable}.

   El muestra el método resultante en el algoritmo \ref{alg:karatsuba}, en el cual se distinguen las 
   tres secciones correspondientes a los tres sumandos de la expresión \eqref{eq:multiplicacionKaratsuba}.
  \begin{itemize}
    \item Líneas \ref{algkar:1Ini}--\ref{algkar:1End} para el cálculo de $S_{11}$ y $S_{12}$.
    \item Líneas \ref{algkar:2Ini}--\ref{algkar:2End} para el cálculo de $S_{2}$.
    \item Líneas \ref{algkar:3Ini}--\ref{algkar:3End} para el cálculo de $S_{31}$ y $S_{32}$.
  \end{itemize}

  \begin{algorithm} 
  \caption{Multiplicación Karatsuba} \label{alg:karatsuba}
  \begin{algorithmic}[1] 
    \Procedure{Karatsuba}{entero $X$, entero $Y$}
    \State $m \gets \lfloor \frac{\log_2{X}}{2} \rfloor$
    \If{$m > UMBRAL$}
      \State $x_0 \gets X \bmod 2^{m}$
      \State $x_1 \gets \lfloor \frac{X}{2^{m}} \rfloor $
      \State $y_0 \gets Y \bmod 2^{m}$
      \State $y_1 \gets \lfloor \frac{Y}{2^{m}} \rfloor $
      \State
      \State \texttt{\#pragma omp parallel sections} \label{algkar:omp0}
      \State \texttt{\#pragma omp section} \label{algkar:omp1}
      \State $P_1 \gets Karatsuba(x_1, y_1)$ \label{algkar:1Ini}
      \State $S_{11} \gets P_1 \times B^2$
      \State $S_{12} \gets P_1 \times B$  \label{algkar:1End}
      \State
      \State \texttt{\#pragma omp section} \label{algkar:omp2}
      \State $P_2 \gets Karatsuba((x_1 - y_0),(y_0 - y_1))$  \label{algkar:2Ini}
      \State $S_{2} \gets P_2 \times B$ \label{algkar:2End}
      \State
      \State \texttt{\#pragma omp section} \label{algkar:omp3}
      \State $P_3 \gets Karatsuba(x_0, y_0)$  \label{algkar:3Ini}
      \State $S_{31} \gets P_3 \times B$
      \State $S_{32} \gets P_3$ \label{algkar:3End}
      \State
      \State $resultado = (S_11 + S_12) + S_2 + (S_31 + S_32)$
    \Else
      \State Multiplicación clásica
    \EndIf
    \EndProcedure
  \end{algorithmic}
  \end{algorithm}

  \subsection{Comparativa}
    En las figura \ref{fig:karatsuba} se compara la multiplicación de enteros
    mediante los métodos secuencial y paralelo. 
  \begin{figure}[h]
    \centering
    \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{karatsuba} 
    \caption{Comparativa producto enteros}\label{fig:karatsuba}
  \end{figure}
    El ratio entre el tiempo empleado
    por el método paralelo y el empleado por el secuencial se representa en
    \ref{fig:propKaratsuba}, junto con su «suavizado» (aproximación) mediante
    una curva de Bézier.
  \begin{figure}[h]
    \centering
    \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propKaratsuba} 
    \caption{Comparativa producto enteros (proporción)}\label{fig:propKaratsuba}
  \end{figure}
  Tal como se puede observar, la proporción está siempre por debajo de $1$, por lo que
  el método parelelo resulta ventajoso para todo valor. Nótese que el método de Karatsuba
  se emplea únicamente a partir de factores de una cierto tamaño (véase \cite{miproyecto}),
  lo cual explica este hecho. Asimismo, el ratio parece tender a $0.7$, es decir, una 
  ganancia de $1/0.7 \approx 1.43$ y una eficiencia de $1.43 / 2 \approx 0.715$. Debe tenerse en cuenta que el número
  de unidades de procesamiento del sistema en el que se han realizado las mediciones es de dos, mientras
  que el número de tareas es de tres: nos encontramos limitados por nuestro sistema físico.
  %TODO



\section{Potenciación modular}
  El cálculo de $x = b^e \pmod{m}$ es básico en criptografía. Métodos tales como el RSA se apoyan en esta
  operación. Desafortunadamente, el enfoque secuencial está basado en la mayoría de los casos
  en variantes del método de exponenciación binaria. Tal es el caso de la anterior
  versión de esta biblioteca: véase \cite{miproyecto}, sección 5.5.3, pág. 64. Es fácil observar cómo este
  método impone una cadena de dependencias que da al traste con la paralelización del algoritmo de forma
  más o menos directa. Se hace necesario por tanto basarse en algún otro principio.

  \subsection{Dos hilos y $\mathbb{Z}M_n$}
  Curiosamente, dada la importancia de la operación, no parece existir en el momento de elaborar esta memoria 
  demasiada bibliografía al respecto. Una excepción es \cite{modexpmont}, donde se expone un método basado en
  dividir la exponenciación en dos: un proceso trabaja sobre la base original; el otro sobre la inversa modular
  de la base. Los detalles concretos del método pueden consultarse en la anterior referencia bibliográfica. 

  Adolece sin embargo de lo siguiente:
  \begin{itemize}
    \item Requiere el cálculo de la inversa modular. Esta es una operación costosa.
    \item Se limita a la mínima expresión de paralelismo: dos procesos.
  \end{itemize}

  Como forma de mitigar el hecho de requerir la inversa modular, \cite{modexpmont} sugiere el uso 
  de la inversa de Montgomery, concepto introducido en la definición \ref{def:invMont}, página \pageref{def:invMont}.

  En la biblioteca este método ha sido implementado, utilizando enteros modulares en el dominio de Montgomery 
  --sección \ref{sec:zm}--, en la clase \texttt{TwoThreadedModularExp}.

  \subsection{El método de la casa}
  Debido a que el anterior método dejaba que desear, sobre todo en cuanto a escalabilidad, se hemos
   tratado de aportar nuestro granito de arena mediante una estrategia desarrollada de forma original.

    Considerando la exponenciación como $x = b^e \pmod{m}$, sea $l = bitLength(e)$ la longitud
    en bits del exponente y $k = \lfloor \frac{l}{\# hilos} \rfloor$ el número de bits
    por elemento de una (equi)partición de $e$. Las partes son nombradas $s_0, \ldots, s_{k-1}$,
    tal como se muestra en la figura \ref{fig:miModExp}.

   \begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\textwidth,keepaspectratio]{expMiModExp} 
    \caption{Particionado del exponente}\label{fig:miModExp}
  \end{figure}

  Así pues, es posible definir:
  \begin{eqnarray}
    b^e & = & b^{s_0 + 2^k s_1 + 2^{2k} s_2 + \cdots + 2^{(k-1)k} s_{k-1} } = \nonumber \\
        & = & b^{s_0} \times (b^{2^k})^{s_1} \times \ldots \times (b^{2^{(k-1)^k}})^{s_{k-1}} \label{eq:miExp}
  \end{eqnarray}
  Lo interesante de esta formulación es que cada factor de la expresión \eqref{eq:miExp} puede ser calculado
  de forma independiente. 

  Es preciso resaltar algunos detalles: 
  \begin{itemize}
    \item El exponente del  último factor de la expresión \eqref{eq:miExp} tiene el mismo número de bits
    que el exponente $e$ original.
    \item El «ajuste de los exponentes» necesario en todos los $s_i$ excepto para $s_0$ puede realizarse
    mediante desplazamientos de bits.
    \item El algoritmo paralelo sigue requiriendo el uso de algún método secuencial para la exponenciación
    de los diferentes componentes individuales. Esto en realidad aporta flexibilidad al método.
  \end{itemize}
  El primer punto anterior puede hacer pensar que este proceso es un remedio peor que la propia enfermedad. Pero sin 
 embargo es posible sacar ventaja basándose en la especial estructura de los exponentes para los diferentes $s_i$. 

 \subsubsection{Análisis de complejidad}
  Sea $T(e)$ el tiempo requerido para un exponente $e$, $t = bitLength(e)$ y $wt = \textrm{número de unos en } e$. 
  Utilizando un enfoque secuencial basado en el método de exponenciación binaria, \cite{handbook}\footnote{sección 14.6.1}
  analiza que es necesario el cálculo de $t$ cuadrados y $wt$ productos. Dado que ambas operaciones están en $O(n^2)$, 
  $T(e) = t O(n^2) + wt O(n^2) = (wt + t)O(n^2)$.

  Para la versión paralela dada por la expresión \eqref{eq:miExp}, 
  \[
    T(e) = \sum^{k-1}_{i=0}\left( T(s_i) \right) + T_m
  \]
  donde $T_m$ representa el tiempo requerido para el cálculo de productorio que combina el resultado
  de las exponenciaciones individuales sobre los $s_i$.
  
  Cada una de las partes $s_i$ supone 
  $T(s_i) = (wt_i + t_i)O(n^2)$, con $wt_i \approx wt/k$ y $t_i = (t/k)i$. Como se ha indicado anteriormente,
  el peor caso se da cuando $i=k$, y el exponente para $s_{k-1}$ tiene el mismo número de bits que $e$, es decir, $t_{k-1} = t$:
  $T(s_{k-1}) = (wt_{k-1} + t)O(n^2) = \underbrace{wt_{k-1}O(n^2)}_{< wt O(n^2)} + t O(n^2) \Rightarrow T(s_{k-1}) < T(e)$.
  Para cualquier otro $i < k-1$, se cumple asimismo $T(s_i) < T(e)$. 

  El último punto por analizar es el término $T_m$. Éste se encuentra en $O(n^{1.58})$ si, tal como es el caso, el producto
  de enteros se implementa mediante el método de Karatsuba (véase \cite{miproyecto}\footnote{pág. 50}). 
  
  La complejidad para la versión paralela del algoritmo es por tanto asintóticamente menor que la versión secuencial,
  tanto en cuanto todos los procesos individuales que conforman la versión paralela lo son, y $T_m$ aporta una complejidad
  menor a $O(n^2)$. 

  Desafortunadamente, en el plano práctico y debido principalmente a que no nos ha sido posible tener acceso a equipos
  con más de dos unidades de ejecución, en las pruebas se han obtenido tiempos por debajo de la versión secuencial. Sería de esperar
  que con un mayor número de unidades de ejecución, el mayor grado de paralelismo compensase las constantes ocultas que hacen
  que aún no podamos batir a la versión secuencial.


\section{Vectores}
  \subsection{Producto escalar}

\section{Matrices}
  \subsection{Producto}\label{par:matprod}

  \subsubsection{Comparativa}

    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{productoMat} 
      \caption{Comparativa producto matricial }\label{fig:prodMat}
    \end{figure}

    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propProdMat} 
      \caption{Comparativa producto matricial (proporción)}\label{fig:propProdMat}
    \end{figure}



  \subsection{Descomposición $LU$}\label{par:lu}
 
     Se ha utilizado el método de Doolittle para la descomposición $LU$. Se presenta
     su versión en pseudocódigo en el listado \ref{alg:doolittle}. 

    \begin{algorithm}
      \caption{Algoritmo de Doolittle}\label{alg:doolittle}
      \begin{algorithmic}[1]
        \Procedure{Doolittle}{Matriz $M \in \mathcal{M}_{n \times n}$}
        \State $sign \gets 1$
        \For{$i \in \{0, \ldots, n-1\}$}
          \State $p_i = i$ \Comment Inicializar el vector de permutaciones
        \EndFor
        \For{$k \in \{0, \ldots, n-2\}$}
          \If{$M_{kk} = 0$}
            \State $rowPivot \gets$ \textsc{pivot(}$i$\textsc{)} 
            \If{$rowPivot = 0$} \Comment{No se ha podido pivotar}
              \State \textbf{Return} $0$ \Comment{Matriz singular}
            \EndIf
            \State $sign \gets -sign$ \Comment{Se ha permutado una fila}
            \State $p_k \leftrightarrow p_{rowPivot}$ \Comment{Registrar la permutación de filas}
          \EndIf
          \State \texttt{\#pragma omp parallel for} \label{algdoo:omp}
          \For{$i \in \{k+1,\ldots,n-1\}$}
            \State $M_{ik} \gets M_{ik} / M_{kk}$
            \For{$j \in \{k+1,\ldots,n-1\}$}
              \State $M_{i,j} \gets M_{i,j} - (M_{i,k}*M_{k,j}) $
            \EndFor %for j
          \EndFor %for i
        \EndFor %for k
        \State \textbf{Return } $(sign, p)$
        \EndProcedure
      \end{algorithmic}
    \end{algorithm}

    La justificación de las directivas OpenMP en la línea \ref{algdoo:omp} derivan del 
    examen del patrón de acceso a los datos representado en la figura \ref{fig:accDoo}. 
    \begin{figure}[h]
      \centering
      \includegraphics[width=0.85\textwidth,keepaspectratio]{patronAccesosDoolittle} 
      \caption{Patrón accesos Doolittle}\label{fig:accDoo}
    \end{figure}
    En ésta se aprecia cómo para cada $k$, cada fila (variable $i$) es procesada
    de forma independiente a las demás. Los datos ajenos a la propia fila $i$ 
    que ésta necesita son de lectura, y no se modifican dentro del bucle que itera
    sobre $i$. No es posible paralelizar sobre $k$ ya que en este caso sí existe
    una cadena de dependencias. Por ejemplo, sobre los elementos de la diagonal. 

  \subsubsection{Comparativa}
    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{solveMat} 
      \caption{Comparativa resolución de sistemas }\label{fig:solveMat}
    \end{figure}

    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propSolveMat} 
      \caption{Comparativa resolución de sistemas (proporción)}\label{fig:propSolveMat}
    \end{figure}
  \subsection{Inversión}

  \subsubsection{Comparativa}
    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{inversionMat} 
      \caption{Comparativa inversión matricial }\label{fig:invMat}
    \end{figure}

    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propInvMat} 
      \caption{Comparativa inversión matricial (proporción)}\label{fig:propInvMat}
    \end{figure}



  \subsection{Determinante}
    \subsubsection{Algoritmo de Gauss-Bareiss}\label{para:bareiss}
      Descrito en la sección \ref{impl:bareiss}. Si se representa el patrón de accesos derivado 
      de su algoritmo secuencial (algoritmo \ref{alg:bareiss}), no es difícil derivar una división
      en tareas basada en la secuencia de patrones de diseño representada en la figura \ref{fig:patternDecissionTree}
      de la página \pageref{fig:patternDecissionTree}. En la figura \ref{fig:accBar} 
      se muestra dicho patrón de accesos para el algoritmo.  

      \begin{figure}[h]
       \centering
       \includegraphics[width=0.85\textwidth,keepaspectratio]{patronAccesosBarreis} 
       \caption{Patrón accesos Gauss-Bareiss}\label{fig:accBar}
      \end{figure}

      De este patrón de acceso se deriva una paralelización organizada por el acceso a los datos y
      con una enumeración lineal. Por tanto el patrón estructural más indicado parece ser una descomposición
      geométrica sobre las filas, siguiendo el recorrido de $k$. A nivel de estructura de soporte, el patrón de 
      paralelismo de bucle se ajusta perfectamente: 

      \begin{algorithm}
        \caption{Algoritmo de Gauss-Bareiss}\label{alg:bareiss}
        \begin{algorithmic}[1]
          \Procedure{Gauss-Bareiss}{Matriz $M \in \mathcal{M}_{n \times n}$}
          \State $sign \gets 1$
          \For{$i \in \{0, \ldots, n-2\}$}
            \State $p \gets M_{i,i}$
            \If{$p = 0$}
              \If{$ \neg $\textsc{pivot(}$i$\textsc{)} } \Comment{Si \textrm{pivot} es falso, no se ha podido pivotar}
                \State \textbf{Return} $0$ \Comment{Matriz singular}
              \EndIf
              \State $sign \gets -sign$ \Comment{Se ha permutado una fila}
              \State $p \gets M_{i,i}$ \Comment{$M_{i,i} \neq 0$}
            \EndIf
            \State \texttt{\#pragma omp parallel}
            \For{$j \in \{i+1,\ldots,n-1\}$}
              \State \texttt{\#pragma omp for}
              \For{$k \in \{i+1,\ldots,n-1\}$}
                \State $M_{j,k} \gets M_{j,k} * p - M_{j,i}*M_{i,k}$
                \If{ $i > 0$ }
                  \State $M_{j,k} \gets M_{j,k} / M_{i-1,i-1}$ \Comment{Esta división \emph{siempre} es exacta}
                \EndIf
              \EndFor %for k
            \EndFor %for j
          \EndFor %for i

          \State \textbf{Return } $sign * M_{n-1,n-1}$
          \EndProcedure
        \end{algorithmic}
      \end{algorithm}

    \paragraph{Comparativa}

      \begin{figure}[h]
        \centering
        \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{barreis} 
        \caption{Comparativa Gauss-Bareiss}\label{fig:bareiss}
      \end{figure}

      \begin{figure}[h]
        \centering
        \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propBarreis} 
        \caption{Comparativa Gauss-Bareiss (proporción)}\label{fig:propBarreis}
      \end{figure}


%  \subsection{Operaciones de carácter vectorial}
    % sumas, restas, prod por escalares

\section{Cálculo de \textit{reducciones}}
  \subsection{Máximo común divisor de una colección de enteros}


\section{Evaluación de polinomios}\label{parallel:evalPoly}
%  \subsection{Operaciones de carácter vectorial}
%    % sumas, restas, prod por escalares
  La evaluación de polinomios por medio de la regla de Horner, formulada en la sección \ref{tipos:evaluacionPoly}, 
  parece presentar una cadena de dependencias insalvables de cara a la paralelización del método. Sin embargo, ya 
  W. S. Dorn en $1962$ (\cite{dorn}) presenta una generalización de la regla de Horner que permite, en teoría, la 
  evaluación de un polinomio de forma paralela sobre $k$ unidades de procesamiento. Este mismo método es también 
  citado por \cite{knuth}\footnote{sección 4.6.4, pág. 487}. En la práctica, el método general no es práctico, pero sí
  lo son las particularizaciones de orden $2$, $3$ y $4$, en lo que a nosotros nos concierne: es más, el sistema
  de desarrollo cuenta únicamente con dos unidades de ejecución, por lo que el siguiente análisis se centra en el
  desarrollo de Horner de orden $2$. 

  Recuérdese como se obtiene la regla de Horner clásica, dada en la página \pageref{horner}: sea $u(x) = u_n x^n + \cdots + u_1 x + u_0$.
  Dividiendo $u(x)$ entre $x-x_0$, $u(x) = (x-x_0)q(x)+r(x)$. Dado que el grado del polinomio resto $\mathrm{grado}(r(x)) < 1$,
  $r(x)$ será una constante. Asimismo, $u(x_0) = 0 \cdot q(x_0) + r = r$. Si se examina detenidamente, se observa
  que este es el principio en el que se sostiene la regla de Horner. 
  
  En general, si se divide $u(x)$ entre $f(x)$, 
  se obtiene $u(x) = f(x)q(x) + r(x)$, y si $f(x_0) = 0$, se cumple $u(x_0) = r(x_0)$. Si se toma $f(x) = x^2 - x^2_0$, 
  se obtiene la regla de Horner «de segundo grado»:
  \begin{eqnarray}
    u(x) & = & \nonumber \\
         & = & \left( \ldots(u_{2 \lfloor n/2 \rfloor} x^2 + u_{2 \lfloor n/2 \rfloor - 2})x^2 + \cdots \right) u_0 +  \\ \label{eq:polyeval1}
         & + & \left( (\ldots (u_{2\lceil n/2 \rceil-1} x^2 + u_{2\lceil n/2 \rceil -3})x^2 + \cdots )x^2 + u_1 \right) x \label{eq:polyeval2}
  \end{eqnarray}
  El número de multiplicaciones y sumas es el mismo que para la formulación original. Ahora bien, los sumandos
  \eqref{eq:polyeval1} y \eqref{eq:polyeval2} en la anterior expresión son \emph{independientes}. Es posible
  computarlos por separado, y de ahí se obtiene el paralelismo. Además, la carga computacional de ambos es prácticamente
  la misma, por lo que se espera un buen comportamiento en cuanto a la distribución de carga.

  Para el lector interesado, la regla de Horner de orden $k$-ésimo se obtiene con un polinomio $f(x) = x^k - x^k_0$. Para
  más detalles, véanse las referencias bibliográficas referidas al inicio de este punto.

  \subsection{Comparativa}
    En la figura \ref{fig:polypar} se muestra una comparativa de tiempos de ejecución para la evaluación
    de varios polinomios, en paralelo y secuencialmente. 
    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{poly} 
      \caption{Comparativa evaluación polinomio}\label{fig:polypar}
    \end{figure}
    Independientemente de la comparativa, resulta curioso observar los picos que se producen para ciertos
    grados del polinomio. Este comportamiento, aunque atenuado, se aprecia también para el cálculo en paralelo. 
    Aunque se desconoce su razón concreta, es posible que tenga que ver con la política particular de gestión de cachés del
    procesador.

    Para facilitar la comparativa, se presenta en la figura \ref{fig:propPoly} la proporción paralelo-secuencial para
    este método.
    \begin{figure}[h]
      \centering
      \includegraphics[angle=270,width=0.85\textwidth,keepaspectratio]{propPoly} 
      \caption{Comparativa evaluación polinomio (proporción)}\label{fig:propPoly}
    \end{figure}
    Se aprecia una tendencia hacia $\approx 0.55$, lo que da una ganancia de $1/0.55 \approx 1.82$;
    y una eficiencia de $1.82 / 2 = 0.91$, lo cual es muy buen resultado.


